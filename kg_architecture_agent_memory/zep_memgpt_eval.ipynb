{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "######## Installations\n",
    "\n",
    "!pip install zep-cloud openai --upgrade"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmL6bBy1yam6",
    "outputId": "95667fee-e18c-4fee-de93-90f16769369c",
    "ExecuteTime": {
     "end_time": "2025-01-21T14:46:42.871662Z",
     "start_time": "2025-01-21T14:46:40.338286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zep-cloud in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (2.1.1)\r\n",
      "Collecting zep-cloud\r\n",
      "  Downloading zep_cloud-2.3.1-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: openai in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (1.54.4)\r\n",
      "Collecting openai\r\n",
      "  Downloading openai-1.59.9-py3-none-any.whl.metadata (27 kB)\r\n",
      "Requirement already satisfied: httpx>=0.21.2 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from zep-cloud) (0.27.2)\r\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from zep-cloud) (2.9.2)\r\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from zep-cloud) (4.11.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from openai) (4.6.2)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from openai) (1.8.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from openai) (0.7.1)\r\n",
      "Requirement already satisfied: sniffio in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from openai) (4.67.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\r\n",
      "Requirement already satisfied: certifi in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from httpx>=0.21.2->zep-cloud) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from httpx>=0.21.2->zep-cloud) (1.0.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.21.2->zep-cloud) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from pydantic>=1.9.2->zep-cloud) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/prestonrasmussen/miniconda3/lib/python3.12/site-packages (from pydantic>=1.9.2->zep-cloud) (2.23.4)\r\n",
      "Downloading zep_cloud-2.3.1-py3-none-any.whl (82 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.5/82.5 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading openai-1.59.9-py3-none-any.whl (455 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m455.5/455.5 kB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: zep-cloud, openai\r\n",
      "  Attempting uninstall: zep-cloud\r\n",
      "    Found existing installation: zep-cloud 2.1.1\r\n",
      "    Uninstalling zep-cloud-2.1.1:\r\n",
      "      Successfully uninstalled zep-cloud-2.1.1\r\n",
      "  Attempting uninstall: openai\r\n",
      "    Found existing installation: openai 1.54.4\r\n",
      "    Uninstalling openai-1.54.4:\r\n",
      "      Successfully uninstalled openai-1.54.4\r\n",
      "Successfully installed openai-1.59.9 zep-cloud-2.3.1\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lHIC512UV51K",
    "ExecuteTime": {
     "end_time": "2025-01-21T15:06:48.971775Z",
     "start_time": "2025-01-21T15:06:48.966399Z"
    }
   },
   "source": [
    "######## Imports\n",
    "\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "from zep_cloud.client import AsyncZep\n",
    "from zep_cloud import Message, EntityEdge, EntityNode\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "from datetime import datetime, timezone"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "ZEP_API_KEY = os.getenv('ZEP_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "id": "t-bBAEydL9su",
    "ExecuteTime": {
     "end_time": "2025-01-21T15:06:50.049572Z",
     "start_time": "2025-01-21T15:06:50.046800Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "######## Helper functions\n",
    "\n",
    "def nicely_display_one_data_sample(msc_dataset_df, idx):\n",
    "    \"\"\"\n",
    "    Given the MSC dataset's dataframe and an idx (between 0 and 499), print out one data sample\n",
    "    in a readable format.\n",
    "    \"\"\"\n",
    "\n",
    "    msc_dataset_df_keys = msc_dataset_df.keys()\n",
    "\n",
    "    for key in msc_dataset_df_keys:\n",
    "\n",
    "        keys_object = msc_dataset_df[key][idx]\n",
    "        print(f\"\\n*****{key}\")\n",
    "\n",
    "        # Handle metadata and self instruct differently\n",
    "        if key == \"metadata\":\n",
    "            continue\n",
    "        if key == \"self_instruct\":\n",
    "            print(\"QUESTION (B's Message):\", keys_object['B'])\n",
    "            print(\"ANSWER (A's Message):\", keys_object['A'])\n",
    "\n",
    "        for i in range(20):\n",
    "            try:\n",
    "                print(keys_object[i])\n",
    "            except:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "def get_all_chat_sessions(msc_dataset_df, idx):\n",
    "    \"\"\"\n",
    "    Given the MSC dataset's dataframe and an idx (between 0 and 499), return a list of all\n",
    "    5 chat sessions in a nice format, only including the information needed for Zep.\n",
    "    \"\"\"\n",
    "    all_chat_sessions = []\n",
    "\n",
    "    # Gather the previous chat sessions\n",
    "    for i, prev_dialog in enumerate(msc_dataset_df[\"previous_dialogs\"][idx]):\n",
    "        prev_dialog_message_list = []\n",
    "        for msg_dict in prev_dialog[\"dialog\"]:\n",
    "            prev_dialog_message_list.append(msg_dict[\"text\"])\n",
    "        all_chat_sessions.append({\n",
    "             \"messages\": prev_dialog_message_list,\n",
    "             \"session_num\": i + 1,\n",
    "             \"time_num\": prev_dialog[\"time_num\"],\n",
    "             \"time_unit\": prev_dialog[\"time_unit\"],\n",
    "             \"time_back\": prev_dialog[\"time_back\"]\n",
    "        })\n",
    "\n",
    "\n",
    "    # Gather the most recent/newest chat session, always session #5\n",
    "    newest_dialog = msc_dataset_df[\"dialog\"][idx]\n",
    "    newest_dialog_message_list = []\n",
    "    for msg_dict in newest_dialog:\n",
    "        newest_dialog_message_list.append(msg_dict[\"text\"])\n",
    "    all_chat_sessions.append({\"messages\": newest_dialog_message_list, \"session_num\": 5})\n",
    "\n",
    "    return all_chat_sessions\n",
    "\n"
   ],
   "metadata": {
    "id": "gQN9V9UlXXR5",
    "ExecuteTime": {
     "end_time": "2025-01-21T15:06:54.276449Z",
     "start_time": "2025-01-21T15:06:54.271223Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "######## Download the eval dataset from the official HuggingFace Source\n",
    "# Download this file and store it in data/msc.jsonl\n",
    "# https://huggingface.co/datasets/MemGPT/MSC-Self-Instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "######## Load the eval dataset\n",
    "\n",
    "msc_dataset_df = pd.read_json('data/msc.jsonl', lines=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_t5PZzC8M66d",
    "outputId": "d2ed3ca8-e245-4d9b-aa8f-accb550486ed",
    "ExecuteTime": {
     "end_time": "2025-01-21T14:46:57.203033Z",
     "start_time": "2025-01-21T14:46:56.943065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cf/0l2z23596d998gx8ynry53yc0000gn/T/ipykernel_41777/4234614155.py:3: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  msc_dataset_df = pd.read_json('data/msc.jsonl', lines=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m######## Load the eval dataset\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m msc_dataset_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_json(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/msc.jsonl\u001B[39m\u001B[38;5;124m'\u001B[39m, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:815\u001B[0m, in \u001B[0;36mread_json\u001B[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m json_reader\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 815\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m json_reader\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:1023\u001B[0m, in \u001B[0;36mJsonReader.read\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1021\u001B[0m         data \u001B[38;5;241m=\u001B[39m ensure_str(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata)\n\u001B[1;32m   1022\u001B[0m         data_lines \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1023\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_object_parser(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_lines(data_lines))\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1025\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_object_parser(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:1051\u001B[0m, in \u001B[0;36mJsonReader._get_object_parser\u001B[0;34m(self, json)\u001B[0m\n\u001B[1;32m   1049\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframe\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1051\u001B[0m     obj \u001B[38;5;241m=\u001B[39m FrameParser(json, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\u001B[38;5;241m.\u001B[39mparse()\n\u001B[1;32m   1053\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseries\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1054\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, \u001B[38;5;28mbool\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:1187\u001B[0m, in \u001B[0;36mParser.parse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1185\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m-> 1187\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse()\n\u001B[1;32m   1189\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1190\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:1403\u001B[0m, in \u001B[0;36mFrameParser._parse\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1399\u001B[0m orient \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39morient\n\u001B[1;32m   1401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m orient \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1402\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj \u001B[38;5;241m=\u001B[39m DataFrame(\n\u001B[0;32m-> 1403\u001B[0m         ujson_loads(json, precise_float\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecise_float), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1404\u001B[0m     )\n\u001B[1;32m   1405\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m orient \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1406\u001B[0m     decoded \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   1407\u001B[0m         \u001B[38;5;28mstr\u001B[39m(k): v\n\u001B[1;32m   1408\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m ujson_loads(json, precise_float\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecise_float)\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   1409\u001B[0m     }\n",
      "\u001B[0;31mValueError\u001B[0m: Expected object or value"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "######## Start up Zep and OpenAI clients\n",
    "zep = AsyncZep(api_key=os.getenv(\"ZEP_API_KEY\"), base_url=\"https://api.development.getzep.com/api/v2\")\n",
    "oai_client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ],
   "metadata": {
    "id": "F3X8rQyVLRJf",
    "ExecuteTime": {
     "end_time": "2025-01-21T14:47:21.341017Z",
     "start_time": "2025-01-21T14:47:21.314332Z"
    }
   },
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "status_code: None, body: The client must be instantiated be either passing in api_key or setting ZEP_API_KEY",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mApiError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m######## Start up Zep and OpenAI clients\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m zep \u001B[38;5;241m=\u001B[39m AsyncZep(api_key\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mZEP_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m), base_url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://api.development.getzep.com/api/v2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m oai_client \u001B[38;5;241m=\u001B[39m AsyncOpenAI(\n\u001B[1;32m      4\u001B[0m     api_key\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      5\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/zep_cloud/client.py:53\u001B[0m, in \u001B[0;36mAsyncZep.__init__\u001B[0;34m(self, base_url, environment, api_key, timeout, follow_redirects, httpx_client)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m env_api_url:\n\u001B[1;32m     52\u001B[0m     base_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_api_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/api/v2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     54\u001B[0m     base_url\u001B[38;5;241m=\u001B[39mbase_url,\n\u001B[1;32m     55\u001B[0m     environment\u001B[38;5;241m=\u001B[39menvironment,\n\u001B[1;32m     56\u001B[0m     api_key\u001B[38;5;241m=\u001B[39mapi_key,\n\u001B[1;32m     57\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m     58\u001B[0m     follow_redirects\u001B[38;5;241m=\u001B[39mfollow_redirects,\n\u001B[1;32m     59\u001B[0m     httpx_client\u001B[38;5;241m=\u001B[39mhttpx_client\n\u001B[1;32m     60\u001B[0m )\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory \u001B[38;5;241m=\u001B[39m AsyncMemoryClient(client_wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_wrapper)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocument \u001B[38;5;241m=\u001B[39m AsyncDocumentClient(client_wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_wrapper)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/zep_cloud/base_client.py:134\u001B[0m, in \u001B[0;36mAsyncBaseClient.__init__\u001B[0;34m(self, base_url, environment, api_key, timeout, follow_redirects, httpx_client)\u001B[0m\n\u001B[1;32m    132\u001B[0m _defaulted_timeout \u001B[38;5;241m=\u001B[39m timeout \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m60\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m httpx_client \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m api_key \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ApiError(body\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe client must be instantiated be either passing in api_key or setting ZEP_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_wrapper \u001B[38;5;241m=\u001B[39m AsyncClientWrapper(\n\u001B[1;32m    136\u001B[0m     base_url\u001B[38;5;241m=\u001B[39m_get_base_url(base_url\u001B[38;5;241m=\u001B[39mbase_url, environment\u001B[38;5;241m=\u001B[39menvironment),\n\u001B[1;32m    137\u001B[0m     api_key\u001B[38;5;241m=\u001B[39mapi_key,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    143\u001B[0m     timeout\u001B[38;5;241m=\u001B[39m_defaulted_timeout,\n\u001B[1;32m    144\u001B[0m )\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocument \u001B[38;5;241m=\u001B[39m AsyncDocumentClient(client_wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_wrapper)\n",
      "\u001B[0;31mApiError\u001B[0m: status_code: None, body: The client must be instantiated be either passing in api_key or setting ZEP_API_KEY"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "######## MSC Ingestion loop - ingest messages for each multi-session\n",
    "async def ingest_msc(zep: AsyncZep):\n",
    "    num_multi_sessions = 500\n",
    "    \n",
    "    for multi_session_idx in range(num_multi_sessions):\n",
    "        \n",
    "        if multi_session_idx < 0:\n",
    "            continue\n",
    "    \n",
    "        # Create a unique Zep user and session for this pair of speakers\n",
    "        user_id = \"msc_experiment_user_\" + str(multi_session_idx)\n",
    "        session_id = \"msc_experiment_session_\" + str(multi_session_idx)\n",
    "        \n",
    "        # Uncomment to delete existing users\n",
    "        # try:\n",
    "        #     await zep.user.delete(user_id)\n",
    "        #     await zep.memory.delete(session_id)\n",
    "        # except:\n",
    "        #     pass\n",
    "        # \n",
    "        # continue\n",
    "        \n",
    "        await zep.user.add(user_id=user_id)\n",
    "        await zep.memory.add_session(\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "        )\n",
    "    \n",
    "        # Now add the messages as episodes to this user-session\n",
    "    \n",
    "        all_chat_sessions = get_all_chat_sessions(msc_dataset_df, multi_session_idx)\n",
    "        for session_idx, session in enumerate(all_chat_sessions):\n",
    "            for msg_idx, msg in enumerate(session[\"messages\"]):\n",
    "                if msg_idx % 2 == 0:\n",
    "                    await zep.memory.add(session_id=session_id, messages=[Message(role=\"A\", role_type=\"norole\", content=msg)])\n",
    "                else:\n",
    "                    await zep.memory.add(session_id=session_id, messages=[Message(role=\"B\", role_type=\"norole\", content=msg)])\n",
    "    \n",
    "await ingest_msc(zep)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "Y9Iy7VQ7wcap",
    "outputId": "77a6de57-427f-40e3-c579-eb0d206f0a72"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:13:54.467049Z",
     "start_time": "2025-01-21T15:13:54.457483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "######## Define prompts for Deep Memory Retrieval (DMR) eval\n",
    "async def dmr_response(llm_client, context: str, question: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "        You are speaker A and should respond to all questions in the the first person perspective of A\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            Your task is to briefly answer the question. You are given the following context from the previous conversation. If you don't know how to answer the question, abstain from answering.\n",
    "                <CONTEXT>\n",
    "                {context}\n",
    "                </CONTEXT>\n",
    "                <QUESTION>\n",
    "                {question}\n",
    "                </QUESTION>\n",
    "\n",
    "            Respond with an ANSWER section containing your answer. As well as an EVIDENCE section containing the context that help you came to your conclusion, and an explanation of why that context is relevant.\n",
    "            \"\"\"\n",
    "\n",
    "    response = await llm_client.chat.completions.create(\n",
    "                model='gpt-4-mini',\n",
    "                messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "            )\n",
    "    result = response.choices[0].message.content or ''\n",
    "\n",
    "    return result\n",
    "\n",
    "class Grade(BaseModel):\n",
    "  is_correct: bool = Field(description='Whether or not the response is correct')\n",
    "\n",
    "async def dmr_grader(llm_client, question: str, gold_answer: str, response: str) -> bool:\n",
    "    system_prompt = \"\"\"\n",
    "        You are an expert grader that determines if answers to questions match a gold standard answer\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"         \n",
    "    I will give you a question, a correct answer, and a response from a model. Please answer true if the response contains the correct answer or touches on the same topic. Otherwise, answer false. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer true. If the response only contains a subset of the information required by the answer, answer false.\n",
    "            \n",
    "    <QUESTION>\n",
    "    B: {question}\n",
    "    </QUESTION>\n",
    "    <CORRECT ANSWER>\n",
    "    {gold_answer}\n",
    "    </CORRECT ANSWER>\n",
    "    <RESPONSE>\n",
    "    A: {response}\n",
    "    </RESPONSE>\n",
    "    \"\"\"\n",
    "\n",
    "    response = await llm_client.beta.chat.completions.parse(\n",
    "                model='gpt-4o-mini',\n",
    "                messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": prompt}],\n",
    "                response_format=Grade,\n",
    "                temperature=0,\n",
    "            )\n",
    "    result = response.choices[0].message.parsed\n",
    "\n",
    "    return result.is_correct"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "######## Baseline eval loop - for each multi-session, evaluate on summary and full context\n",
    "async def dmr_baseline():\n",
    "    num_multi_sessions = 500\n",
    "    eval_results = {\n",
    "                    \"total\": num_multi_sessions,\n",
    "                    \"full_conversation_correct\": 0, \n",
    "                    \"full_conversation_accuracy\": 0, \n",
    "                    \"conversation_summary_correct\": 0, \n",
    "                    \"conversation_summary_accuracy\": 0,\n",
    "                  }\n",
    "    \n",
    "    for multi_session_idx in range(num_multi_sessions):\n",
    "        # Build contexts\n",
    "        full_conversation = ''\n",
    "    \n",
    "        all_chat_sessions = get_all_chat_sessions(msc_dataset_df, multi_session_idx)\n",
    "        for session_idx, session in enumerate(all_chat_sessions):\n",
    "            for msg_idx, msg in enumerate(session[\"messages\"]):\n",
    "                if msg_idx % 2 == 0:\n",
    "                    full_conversation += 'A: ' + msg + '\\n'\n",
    "                else:\n",
    "                    full_conversation += 'B: ' + msg + '\\n'\n",
    "    \n",
    "        speaker_a_summary_set = msc_dataset_df[\"summary_speaker_1\"][multi_session_idx]\n",
    "        speaker_b_summary_set = msc_dataset_df[\"summary_speaker_2\"][multi_session_idx]\n",
    "    \n",
    "        \n",
    "    \n",
    "        speaker_a_summary = '<SPEAKER A SUMMARY>'\n",
    "        for session_summary in speaker_a_summary_set:\n",
    "            for fact in session_summary:\n",
    "                speaker_a_summary += '\\n  ' + fact\n",
    "        speaker_a_summary += '\\n</SPEAKER A SUMMARY>'\n",
    "    \n",
    "        speaker_b_summary = '\\n<SPEAKER B SUMMARY>'\n",
    "        for session_summary in speaker_b_summary_set:\n",
    "            for fact in session_summary:\n",
    "                speaker_b_summary += '\\n  ' + fact\n",
    "        speaker_b_summary += '\\n</SPEAKER B SUMMARY>'\n",
    "    \n",
    "        conversation_summary = speaker_a_summary + speaker_b_summary\n",
    "    \n",
    "    \n",
    "        # Set question and golden answer\n",
    "        question = msc_dataset_df['self_instruct'][multi_session_idx][\"B\"]\n",
    "        gold_answer = msc_dataset_df['self_instruct'][multi_session_idx][\"A\"]\n",
    "    \n",
    "        # Prompt the LLM to answer the question\n",
    "        full_conversation_response = await dmr_response(oai_client, full_conversation, question)\n",
    "        conversation_summary_response = await dmr_response(oai_client, conversation_summary, question)\n",
    "    \n",
    "        # print('QUESTION: ', question)\n",
    "        print('GOLDEN ANSWER', gold_answer)\n",
    "        print('FULL CONVERSATION RESPONSE: ', full_conversation_response)\n",
    "        print('CONVERSATION SUMMARY RESPONSE: ', conversation_summary_response)\n",
    "    \n",
    "        # Grade responses\n",
    "        full_conversation_grade = await dmr_grader(oai_client, question, gold_answer, full_conversation_response)\n",
    "        conversation_summary_grade = await dmr_grader(oai_client, question, gold_answer, conversation_summary_response)\n",
    "    \n",
    "        if full_conversation_grade:\n",
    "            eval_results[\"full_conversation_correct\"] += 1\n",
    "        if conversation_summary_grade:\n",
    "            eval_results[\"conversation_summary_correct\"] += 1\n",
    "    \n",
    "    eval_results[\"full_conversation_accuracy\"] = eval_results[\"full_conversation_correct\"] / num_multi_sessions\n",
    "    eval_results[\"conversation_summary_accuracy\"] = eval_results[\"conversation_summary_correct\"] / num_multi_sessions\n",
    "    \n",
    "    print(eval_results)\n",
    "await dmr_baseline()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "######## Main eval loop - for each multi-session, evaluate Zep\n",
    "TEMPLATE = \"\"\"\n",
    "FACTS and ENTITIES represent relevant context to the current conversation.\n",
    "\n",
    "# These are the most relevant facts and their valid date ranges\n",
    "# format: FACT (Date range: from - to)\n",
    "<FACTS>\n",
    "{facts}\n",
    "</FACTS>\n",
    "\n",
    "# These are the most relevant entities\n",
    "# ENTITY_NAME: entity summary\n",
    "<ENTITIES>\n",
    "{entities}\n",
    "</ENTITIES>\n",
    "\"\"\"\n",
    "\n",
    "def format_edge_date_range(edge: EntityEdge) -> str:\n",
    "    # return f\"{datetime(edge.valid_at).strftime('%Y-%m-%d %H:%M:%S') if edge.valid_at else 'date unknown'} - {(edge.invalid_at.strftime('%Y-%m-%d %H:%M:%S') if edge.invalid_at else 'present')}\"\n",
    "    return f\"{edge.valid_at if edge.valid_at else 'date unknown'} - {(edge.invalid_at if edge.invalid_at else 'present')}\"\n",
    "\n",
    "\n",
    "def compose_search_context(edges: list[EntityEdge], nodes: list[EntityNode]) -> str:\n",
    "    facts = [f'  - {edge.fact} ({format_edge_date_range(edge)})' for edge in edges]\n",
    "    entities = [f'  - {node.name}: {node.summary}' for node in nodes]\n",
    "    return TEMPLATE.format(facts='\\n'.join(facts), entities='\\n'.join(entities))\n",
    "\n",
    "\n",
    "async def dmr_eval_session(zep: AsyncZep, idx: int) -> bool:\n",
    "    # Set user values\n",
    "        user_id = \"msc_experiment_user_\" + str(idx)\n",
    "        session_id = \"msc_experiment_session_\" + str(idx)\n",
    "    \n",
    "    \n",
    "        # Now we want to prompt an LLM augmented with Zep memory to answer the question\n",
    "        question = msc_dataset_df['self_instruct'][idx][\"B\"]\n",
    "        gold_answer = msc_dataset_df['self_instruct'][idx][\"A\"]\n",
    "    \n",
    "    \n",
    "        # Get relevant facts and entities\n",
    "        edges_results = (await zep.graph.search(user_id=user_id, reranker='cross_encoder', query=question, scope='edges', limit=20)).edges\n",
    "        node_results = (await zep.graph.search(user_id=user_id, reranker='rrf', query=question, scope='nodes', limit=20)).nodes\n",
    "    \n",
    "    \n",
    "        context = compose_search_context(edges_results, node_results)\n",
    "    \n",
    "        # Prompt an LLM with relevant context\n",
    "        response = await dmr_response(oai_client, context, question)\n",
    "    \n",
    "        # Grade responses\n",
    "        grade = await dmr_grader(oai_client, question, gold_answer, response)\n",
    "        \n",
    "        if not grade:\n",
    "            print('IDX: ', idx)\n",
    "            print('CONTEXT: ', context)\n",
    "            print('QUESTION: ', question)\n",
    "            print('GOLDEN ANSWER: ', gold_answer)\n",
    "            print('RESPONSE: ', response)\n",
    "            print('GRADE: ', grade)\n",
    "    \n",
    "        return grade\n",
    "    \n",
    "async def dmr_eval(zep: AsyncZep):\n",
    "    num_multi_sessions = 500\n",
    "    \n",
    "    eval_results = {\"correct\": 0, \"total\": num_multi_sessions, \"accuracy\": 0}\n",
    "    \n",
    "    # grades = list(await asyncio.gather(*[dmr_eval_session(zep, multi_session_idx) for multi_session_idx in range(num_multi_sessions)]))\n",
    "    grades: list[bool] = []\n",
    "    for i in range(num_multi_sessions):\n",
    "        grades.append(await dmr_eval_session(zep, i))\n",
    "    \n",
    "    for grade in grades:\n",
    "        if grade:\n",
    "            eval_results[\"correct\"] += 1\n",
    "    \n",
    "        # Now that we have the results, delete the user\n",
    "        # await zep.user.delete(user_id)\n",
    "    \n",
    "    eval_results[\"accuracy\"] = eval_results[\"correct\"] / num_multi_sessions\n",
    "    \n",
    "    print(eval_results)\n",
    "    \n",
    "await dmr_eval(zep)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
